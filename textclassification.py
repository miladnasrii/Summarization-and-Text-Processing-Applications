# -*- coding: utf-8 -*-
"""TextClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hZOMi4Qi4Pi1-zyS0fd3n1Xq3xDpPFpI
"""

!pip install torch transformers sklearn
!pip install datasets

import torch
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from torch.utils.data import DataLoader, TensorDataset
from datasets import load_dataset

"""Load dataset"""

dataset = load_dataset('imdb')

"""Classification of data into training and test data"""

train_texts, test_texts, train_labels, test_labels = train_test_split(
    dataset['train']['text'], dataset['train']['label'], test_size=0.2)

"""Load BERT model"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

"""Text tokenization"""

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')

"""Create TensorDataset for training and test data"""

train_labels_tensor = torch.tensor(train_labels)
test_labels_tensor = torch.tensor(test_labels)

train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels_tensor)
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels_tensor)

"""Load BERT model for classification"""

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

"""Optimization settings"""

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

optimizer = AdamW(model.parameters(), lr=5e-5)
for epoch in range(3):
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1} complete. Loss: {loss.item()}")

"""Model evaluation"""

model.eval()
all_preds = []
all_labels = []
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

"""Calculate accuracy & Output"""

accuracy = accuracy_score(all_labels, all_preds)
report = classification_report(all_labels, all_preds, target_names=['negative', 'positive'])
print(f"Accuracy: {accuracy}")
print(report)

